{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kBR0wRMJ02no"
      },
      "outputs": [],
      "source": [
        "# 1. Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import yfinance as yf\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import layers, initializers \n",
        "from keras_tuner import HyperModel, BayesianOptimization\n",
        "from pypfopt import EfficientFrontier, risk_models, expected_returns\n",
        "import json\n",
        "import re\n",
        "import pickle\n",
        "import  os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "os.environ['PYTHONHASHSEED'] = '42'\n",
        "os.environ['TF_DETERMINISTIC_OPS'] = '1'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# with open('symbols.json', 'r') as f:\n",
        "#    tickers = json.load(f)\n",
        "\n",
        "# pk_filenames.json = [\"data/ibm.pk1\", \"data/aapl.pk1\"]\n",
        "\n",
        "with open('pk_filenames.json','r') as f:\n",
        "    filenames = json.load(f)\n",
        "\n",
        "# for filename in filenames:\n",
        "#     df = pd.read_pickle(filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xk2-MdQ5tToh"
      },
      "outputs": [],
      "source": [
        "# 2. Data Collection\n",
        "# with open('symbols.json', 'r') as f:\n",
        "#    tickers = json.load(f)\n",
        "\n",
        "# def fetch_data(ticker, start, end):\n",
        "#     data = yf.download(ticker, start=start, end=end)\n",
        "#     return data['Adj Close']\n",
        "\n",
        "# tickers = ['AAPL', 'MSFT', 'GOOGL', 'TSLA', 'NVDA', 'TLT', 'COST', 'WMT', 'BA', 'DIS', 'JPM', 'AMD']\n",
        "all_expected_returns = {}\n",
        "all_data = pd.DataFrame()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to aggregate daily returns into weekly returns\n",
        "def aggregate_returns(data, freq='W'):\n",
        "    return data.resample(freq).ffill().pct_change().dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Walk-forward validation function\n",
        "def walk_forward_validation(data, model, time_step, n_test):\n",
        "    predictions = []\n",
        "    train, test = data[:-n_test], data[-n_test:]\n",
        "    for i in range(n_test):\n",
        "        train_set = pd.concat([train, test[:i]])\n",
        "        X_train, y_train = create_dataset(train_set, time_step)\n",
        "        model.fit(X_train, y_train, epochs=10, verbose=0)\n",
        "        input_data = train_set[-time_step:].values.reshape((1, time_step, 1))\n",
        "        yhat = model.predict(input_data, verbose=0)\n",
        "        predictions.append(yhat[0, 0])\n",
        "    return predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create dataset function to prepare the data for LSTM\n",
        "def create_dataset(data, time_step):\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - time_step):\n",
        "        X.append(data[i:i + time_step])\n",
        "        y.append(data[i + time_step])\n",
        "    return np.array(X), np.array(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for filename in filenames:\n",
        "    df = pd.read_pickle(filename)\n",
        "    # data = fetch_data(ticker, '2020-01-01', '2023-01-01')\n",
        "    data = df['Adj Close']\n",
        "    ticker = filename.split('/')[1].split('.')[0]\n",
        "\n",
        "    # all_data[ticker] = data  # Store data for covariance calculation\n",
        "    \n",
        "\n",
        "    # Data Preprocessing\n",
        "    weekly_returns = aggregate_returns(data)  # Aggregate to weekly returns\n",
        "\n",
        "    # Normalize the weekly returns using Min-Max Scaler\n",
        "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "    weekly_returns_reshaped = weekly_returns.values.reshape(-1, 1)\n",
        "    scaler.fit(weekly_returns_reshaped)\n",
        "    weekly_returns_normalized = scaler.transform(weekly_returns_reshaped)\n",
        "\n",
        "    # Reshape data for LSTM in a compatible sliding window format\n",
        "    time_step = 4\n",
        "    X, y = create_dataset(weekly_returns_normalized, time_step)\n",
        "\n",
        "    # LSTM Modeling with fixed initializers\n",
        "    class LSTMHyperModel(HyperModel):\n",
        "        def build(self, hp):\n",
        "            model = keras.Sequential()\n",
        "            model.add(layers.Input(shape=(time_step, 1)))  # Update input shape\n",
        "            model.add(layers.LSTM(\n",
        "                units=hp.Int('units', min_value=32, max_value=128, step=32),\n",
        "                activation='relu',\n",
        "                kernel_initializer=initializers.GlorotUniform(seed=42),  # Fixed seed for weights\n",
        "                bias_initializer=initializers.Zeros()  # Fixed bias initializer\n",
        "            ))\n",
        "            model.add(layers.Dense(1))  # Ensure the output layer has a fixed size\n",
        "            model.compile(optimizer=keras.optimizers.Adam(hp.Choice('learning_rate', [1e-2, 1e-3])), loss='mse')\n",
        "            return model\n",
        "\n",
        "    # Hyperparameter tuning\n",
        "    tuner = BayesianOptimization(\n",
        "        LSTMHyperModel(),\n",
        "        objective='val_loss',\n",
        "        max_trials=2,\n",
        "        executions_per_trial=1,\n",
        "        directory='lstm_tuning',\n",
        "        project_name=f'portfolio_optimization_{ticker}'\n",
        "    )\n",
        "    # Search for the best hyperparameters\n",
        "    tuner.search(X, y, epochs=10, validation_split=0.2)\n",
        "    \n",
        "    # Use walk-forward validation to evaluate the model\n",
        "    n_test = 52  # Number of weeks to predict\n",
        "    best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "    model = tuner.hypermodel.build(best_hps)\n",
        "    predicted_returns_normalized = walk_forward_validation(pd.Series(weekly_returns_normalized.flatten()), model, time_step, n_test)\n",
        "\n",
        "    # Inverse transform the predicted returns\n",
        "    predicted_returns = scaler.inverse_transform(np.array(predicted_returns_normalized).reshape(-1, 1)).flatten()\n",
        "\n",
        "    # Calculate the total return over the 52 weeks using compounded returns\n",
        "    compounded_return = np.prod(1 + np.array(predicted_returns)) - 1\n",
        "\n",
        "    # Use the compounded return as the annualized expected return\n",
        "    annualized_return = compounded_return\n",
        "    all_expected_returns[ticker] = annualized_return\n",
        "    all_data[ticker] = weekly_returns  # Add this line to populate the all_data dictionary\n",
        "\n",
        "    # Print annualized expected returns for each ticker\n",
        "    print(f\"Annualized Expected Returns for {ticker}: {annualized_return}\")\n",
        "\n",
        "    filename = ticker + '_annualized_return.pkl'\n",
        "    with open(filename,'wb') as f:\n",
        "        pickle.dump(annualized_return,f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "al6N1b8yK2br"
      },
      "outputs": [],
      "source": [
        "# Convert the dictionary to a Pandas Series\n",
        "expected_returns_series = pd.Series(all_expected_returns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cov_matrix = risk_models.risk_matrix(all_data, returns_data=True, method='ledoit_wolf')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qxSsh0jMI_zG",
        "outputId": "0ab34716-c669-4e36-ccc4-aaeb72feec1a"
      },
      "outputs": [],
      "source": [
        "# Portfolio Optimization\n",
        "# Calculate the covariance matrix using all tickers' data\n",
        "# cov_matrix = risk_models.risk_matrix(returns_df, method='ledoit_wolf' )\n",
        "# https://pyportfolioopt.readthedocs.io/en/latest/RiskModels.html\n",
        "ef = EfficientFrontier(expected_returns=expected_returns_series, cov_matrix=cov_matrix)\n",
        "weights = ef.max_sharpe()\n",
        "cleaned_weights = ef.clean_weights()\n",
        "\n",
        "# Print the optimized portfolio weights\n",
        "print(\"Optimized Portfolio Weights:\", cleaned_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open('cleaned_weights.pkl','wb') as f:\n",
        "    pickle.dump(cleaned_weights,f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# VISUALIZATIONS\n",
        "\n",
        "# Dependencies and Libraries\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import pickle\n",
        "from pypfopt import expected_returns, EfficientSemivariance\n",
        "from pypfopt.base_optimizer import BaseConvexOptimizer\n",
        "from pypfopt.objective_functions import ex_post_tracking_error\n",
        "from pypfopt import plotting\n",
        "from pypfopt import risk_models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cleaned Weights and Expected Returns Series\n",
        "with open('cleaned_weights.pkl', 'rb') as f:\n",
        "    cleaned_weights = pickle.load(f)\n",
        "\n",
        "with open('expected_returns_series.pkl', 'rb') as f:\n",
        "    expected_returns_series = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Efficient Frontier\n",
        "ef = EfficientFrontier(mu, S, weight_bounds=(None, None))\n",
        "ef.add_constraint(lambda w: w[0] >= 0.2)\n",
        "ef.add_constraint(lambda w: w[2] == 0.15)\n",
        "ef.add_constraint(lambda w: w[3] + w[4] <= 0.10)\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "plotting.plot_efficient_frontier(ef, ax=ax, show_assets=True)\n",
        "plt.show()\n",
        "\n",
        "# plt.figure(figsize=(10, 6))\n",
        "# ax = plotting.plot_efficient_frontier(ef, show_assets=True)\n",
        "# plt.title('Efficient Frontier')\n",
        "# plt.xlabel('Volatility (Standard Deviation)')\n",
        "# plt.ylabel('Expected Return')\n",
        "# plt.grid(True)\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# **OPTIONAL (EF): Passing a range of parameters (risk, utility, or returns) to generate a frontier**\n",
        "\n",
        "# 100 portfolios with risks between 0.10 and 0.30\n",
        "risk_range = np.linspace(0.10, 0.40, 100)\n",
        "plotting.plot_efficient_frontier(ef, ef_param=\"risk\", ef_param_range=risk_range,\n",
        "                                show_assets=True, showfig=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# **OPTIONAL (EF): Generating more complex plots**\n",
        "\n",
        "# Plotting both the efficient frontier and randomly generated (suboptimal) portfolios, ft. Sharpe ratio:\n",
        "fig, ax = plt.subplots()\n",
        "ef_max_sharpe = ef.deepcopy()\n",
        "plotting.plot_efficient_frontier(ef, ax=ax, show_assets=False)\n",
        "\n",
        "# Finding Tangency Portfolio\n",
        "ef_max_sharpe.max_sharpe()\n",
        "ret_tangent, std_tangent, _ = ef_max_sharpe.portfolio_performance()\n",
        "ax.scatter(std_tangent, ret_tangent, marker=\"*\", s=100, c=\"r\", label=\"Max Sharpe\")\n",
        "\n",
        "# Generating Random Portfolios\n",
        "n_samples = 10000\n",
        "w = np.random.dirichlet(np.ones(ef.n_assets), n_samples)\n",
        "rets = w.dot(ef.expected_returns)\n",
        "stds = np.sqrt(np.diag(w @ ef.cov_matrix @ w.T))\n",
        "sharpes = rets / stds\n",
        "ax.scatter(stds, rets, marker=\".\", c=sharpes, cmap=\"viridis_r\")\n",
        "\n",
        "# Output\n",
        "ax.set_title(\"Efficient Frontier with random portfolios\")\n",
        "ax.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"ef_scatter.png\", dpi=200)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optimized Portfolio Weights\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x=list(cleaned_weights.keys()), y=list(cleaned_weights.values()))\n",
        "plt.title('Optimized Portfolio Weights')\n",
        "plt.xlabel('Assets')\n",
        "plt.ylabel('Weights')\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Expected Annualized Returns\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x=expected_returns_series.index, y=expected_returns_series.values)\n",
        "plt.title('Expected Annualized Returns')\n",
        "plt.xlabel('Assets')\n",
        "plt.ylabel('Expected Return')\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cumulative Portfolio Returns\n",
        "def calculate_portfolio_cumulative_returns(weights, returns_df):\n",
        "    portfolio_returns = (returns_df * weights).sum(axis=1)\n",
        "    cumulative_returns = (1 + portfolio_returns).cumprod() - 1\n",
        "    return cumulative_returns\n",
        "\n",
        "cumulative_returns = calculate_portfolio_cumulative_returns(cleaned_weights, all_data)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(cumulative_returns, label='Portfolio')\n",
        "plt.title('Cumulative Returns of Optimized Portfolio')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Cumulative Return')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Covariance Matrix\n",
        "cov_matrix = risk_models.risk_matrix(all_data, returns_data=True, method='ledoit_wolf')\n",
        "\n",
        "# Converting the covariance matrix to a DataFrame for better visualization\n",
        "cov_matrix_df = pd.DataFrame(cov_matrix, index=all_data.columns, columns=all_data.columns)\n",
        "\n",
        "# Heatmap\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cov_matrix_df, annot=True, fmt=\".2f\", cmap='coolwarm', cbar_kws={'label': 'Covariance'})\n",
        "plt.title('Covariance Matrix Heatmap')\n",
        "plt.xticks(rotation=45)\n",
        "plt.yticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
