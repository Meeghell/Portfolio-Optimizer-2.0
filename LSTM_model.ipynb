{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "kBR0wRMJ02no"
      },
      "outputs": [],
      "source": [
        "# 1. Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import yfinance as yf\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import layers, initializers \n",
        "from keras_tuner import HyperModel, BayesianOptimization\n",
        "from pypfopt import EfficientFrontier, risk_models, expected_returns\n",
        "import json\n",
        "import re\n",
        "import pickle\n",
        "import  os\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "os.environ['PYTHONHASHSEED'] = '42'\n",
        "os.environ['TF_DETERMINISTIC_OPS'] = '1'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# with open('symbols.json', 'r') as f:\n",
        "#    tickers = json.load(f)\n",
        "\n",
        "# pk_filenames.json = [\"data/ibm.pk1\", \"data/aapl.pk1\"]\n",
        "\n",
        "with open('pk_filenames.json','r') as f:\n",
        "    filenames = json.load(f)\n",
        "\n",
        "# for filename in filenames:\n",
        "#     df = pd.read_pickle(filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "xk2-MdQ5tToh"
      },
      "outputs": [],
      "source": [
        "# 2. Data Collection\n",
        "# with open('symbols.json', 'r') as f:\n",
        "#    tickers = json.load(f)\n",
        "\n",
        "# def fetch_data(ticker, start, end):\n",
        "#     data = yf.download(ticker, start=start, end=end)\n",
        "#     return data['Adj Close']\n",
        "\n",
        "# tickers = ['AAPL', 'MSFT', 'GOOGL', 'TSLA', 'NVDA', 'TLT', 'COST', 'WMT', 'BA', 'DIS', 'JPM', 'AMD']\n",
        "all_expected_returns = {}\n",
        "all_data = pd.DataFrame()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to aggregate daily returns into weekly returns\n",
        "def aggregate_returns(data, freq='W'):\n",
        "    return data.resample(freq).ffill().pct_change().dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Walk-forward validation function\n",
        "def walk_forward_validation(data, model, time_step, n_test):\n",
        "    predictions = []\n",
        "    train, test = data[:-n_test], data[-n_test:]\n",
        "    for i in range(n_test):\n",
        "        train_set = pd.concat([train, test[:i]])\n",
        "        X_train, y_train = create_dataset(train_set, time_step)\n",
        "        model.fit(X_train, y_train, epochs=10, verbose=0)\n",
        "        input_data = train_set[-time_step:].values.reshape((1, time_step, 1))\n",
        "        yhat = model.predict(input_data, verbose=0)\n",
        "        predictions.append(yhat[0, 0])\n",
        "    return predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create dataset function to prepare the data for LSTM\n",
        "def create_dataset(data, time_step):\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - time_step):\n",
        "        X.append(data[i:i + time_step])\n",
        "        y.append(data[i + time_step])\n",
        "    return np.array(X), np.array(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reloading Tuner from lstm_tuning\\portfolio_optimization_META\\tuner0.json\n",
            "Annualized Expected Returns for META: 0.448915958404541\n",
            "Reloading Tuner from lstm_tuning\\portfolio_optimization_TSLA\\tuner0.json\n",
            "Annualized Expected Returns for TSLA: 1.464540719985962\n",
            "Reloading Tuner from lstm_tuning\\portfolio_optimization_F\\tuner0.json\n",
            "Annualized Expected Returns for F: 0.1566929817199707\n"
          ]
        }
      ],
      "source": [
        "for filename in filenames:\n",
        "    df = pd.read_pickle(filename)\n",
        "    # data = fetch_data(ticker, '2020-01-01', '2023-01-01')\n",
        "    data = df['Adj Close']\n",
        "    ticker = filename.split('/')[1].split('.')[0]\n",
        "\n",
        "    # all_data[ticker] = data  # Store data for covariance calculation\n",
        "    \n",
        "\n",
        "    # Data Preprocessing\n",
        "    weekly_returns = aggregate_returns(data)  # Aggregate to weekly returns\n",
        "\n",
        "    # Normalize the weekly returns using Min-Max Scaler\n",
        "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "    weekly_returns_reshaped = weekly_returns.values.reshape(-1, 1)\n",
        "    scaler.fit(weekly_returns_reshaped)\n",
        "    weekly_returns_normalized = scaler.transform(weekly_returns_reshaped)\n",
        "\n",
        "    # Reshape data for LSTM in a compatible sliding window format\n",
        "    time_step = 4\n",
        "    X, y = create_dataset(weekly_returns_normalized, time_step)\n",
        "\n",
        "    # LSTM Modeling with fixed initializers\n",
        "    class LSTMHyperModel(HyperModel):\n",
        "        def build(self, hp):\n",
        "            model = keras.Sequential()\n",
        "            model.add(layers.Input(shape=(time_step, 1)))  # Update input shape\n",
        "            model.add(layers.LSTM(\n",
        "                units=hp.Int('units', min_value=32, max_value=128, step=32),\n",
        "                activation='relu',\n",
        "                kernel_initializer=initializers.GlorotUniform(seed=42),  # Fixed seed for weights\n",
        "                bias_initializer=initializers.Zeros()  # Fixed bias initializer\n",
        "            ))\n",
        "            model.add(layers.Dense(1))  # Ensure the output layer has a fixed size\n",
        "            model.compile(optimizer=keras.optimizers.Adam(hp.Choice('learning_rate', [1e-2, 1e-3])), loss='mse')\n",
        "            return model\n",
        "\n",
        "    # Hyperparameter tuning\n",
        "    tuner = BayesianOptimization(\n",
        "        LSTMHyperModel(),\n",
        "        objective='val_loss',\n",
        "        max_trials=2,\n",
        "        executions_per_trial=1,\n",
        "        directory='lstm_tuning',\n",
        "        project_name=f'portfolio_optimization_{ticker}'\n",
        "    )\n",
        "    # Search for the best hyperparameters\n",
        "    tuner.search(X, y, epochs=10, validation_split=0.2)\n",
        "    \n",
        "    # Use walk-forward validation to evaluate the model\n",
        "    n_test = 52  # Number of weeks to predict\n",
        "    best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "    model = tuner.hypermodel.build(best_hps)\n",
        "    predicted_returns_normalized = walk_forward_validation(pd.Series(weekly_returns_normalized.flatten()), model, time_step, n_test)\n",
        "\n",
        "    # Inverse transform the predicted returns\n",
        "    predicted_returns = scaler.inverse_transform(np.array(predicted_returns_normalized).reshape(-1, 1)).flatten()\n",
        "\n",
        "    # Calculate the total return over the 52 weeks using compounded returns\n",
        "    compounded_return = np.prod(1 + np.array(predicted_returns)) - 1\n",
        "\n",
        "    # Use the compounded return as the annualized expected return\n",
        "    annualized_return = compounded_return\n",
        "    all_expected_returns[ticker] = annualized_return\n",
        "    all_data[ticker] = weekly_returns  # Add this line to populate the all_data dictionary\n",
        "\n",
        "    # Print annualized expected returns for each ticker\n",
        "    print(f\"Annualized Expected Returns for {ticker}: {annualized_return}\")\n",
        "\n",
        "    # filename = \"data/\" + ticker + '_annualized_return.pkl'\n",
        "    # with open(filename,'wb') as f:\n",
        "    #     pickle.dump(annualized_return,f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open('data/all_data.pkl','wb') as f:\n",
        "    pickle.dump(all_data,f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "al6N1b8yK2br"
      },
      "outputs": [],
      "source": [
        "# Convert the dictionary to a Pandas Series\n",
        "expected_returns_series = pd.Series(all_expected_returns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "cov_matrix = risk_models.risk_matrix(all_data, returns_data=True, method='ledoit_wolf')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qxSsh0jMI_zG",
        "outputId": "0ab34716-c669-4e36-ccc4-aaeb72feec1a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Optimized Portfolio Weights: OrderedDict([('META', 0.2893), ('TSLA', 0.7107), ('F', 0.0)])\n"
          ]
        }
      ],
      "source": [
        "# Portfolio Optimization\n",
        "# Calculate the covariance matrix using all tickers' data\n",
        "# cov_matrix = risk_models.risk_matrix(returns_df, method='ledoit_wolf' )\n",
        "# https://pyportfolioopt.readthedocs.io/en/latest/RiskModels.html\n",
        "ef = EfficientFrontier(expected_returns=expected_returns_series, cov_matrix=cov_matrix)\n",
        "weights = ef.max_sharpe()\n",
        "cleaned_weights = ef.clean_weights()\n",
        "\n",
        "# Print the optimized portfolio weights\n",
        "print(\"Optimized Portfolio Weights:\", cleaned_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open('data/cleaned_weights.pkl','wb') as f:\n",
        "    pickle.dump(cleaned_weights,f)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
