{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "kBR0wRMJ02no"
      },
      "outputs": [],
      "source": [
        "# 1. Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import yfinance as yf\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.cluster import KMeans\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "from keras_tuner import HyperModel, RandomSearch\n",
        "from pypfopt import EfficientFrontier, risk_models, expected_returns\n",
        "import json\n",
        "import re\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "global cleaned_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "# with open('symbols.json', 'r') as f:\n",
        "#    tickers = json.load(f)\n",
        "\n",
        "# pk_filenames.json = [\"data/ibm.pk1\", \"data/aapl.pk1\"]\n",
        "\n",
        "with open('pk_filenames.json','r') as f:\n",
        "    filenames = json.load(f)\n",
        "\n",
        "# for filename in filenames:\n",
        "#     df = pd.read_pickle(filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "xk2-MdQ5tToh"
      },
      "outputs": [],
      "source": [
        "# 2. Data Collection\n",
        "# with open('symbols.json', 'r') as f:\n",
        "#    tickers = json.load(f)\n",
        "\n",
        "# def fetch_data(ticker, start, end):\n",
        "#     data = yf.download(ticker, start=start, end=end)\n",
        "#     return data['Adj Close']\n",
        "\n",
        "# tickers = ['AAPL', 'MSFT', 'GOOGL', 'TSLA', 'NVDA', 'TLT', 'COST', 'WMT', 'BA', 'DIS', 'JPM', 'AMD']\n",
        "all_expected_returns = {}\n",
        "all_data = pd.DataFrame()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_V1tecOg04ub",
        "outputId": "55f5cf11-3b97-459e-ba43-2ce24a9791a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reloading Tuner from lstm_tuning\\portfolio_optimization_tsla\\tuner0.json\n",
            "\u001b[1m 1/40\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 128ms/step"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\16023\\anaconda3\\envs\\algotrading\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:576: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 12 variables. \n",
            "  saveable.load_own_variables(weights_store.get(inner_path))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
            "Annualized Expected Returns for tsla: 1.074933771013197\n",
            "Reloading Tuner from lstm_tuning\\portfolio_optimization_v\\tuner0.json\n",
            "\u001b[1m 1/40\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 131ms/step"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\16023\\anaconda3\\envs\\algotrading\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:576: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 12 variables. \n",
            "  saveable.load_own_variables(weights_store.get(inner_path))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
            "Annualized Expected Returns for v: 0.1447416123930647\n"
          ]
        }
      ],
      "source": [
        "# Loop through each ticker individually\n",
        "# for ticker in tickers:\n",
        "for filename in filenames:\n",
        "    df = pd.read_pickle(filename)\n",
        "    # data = fetch_data(ticker, '2020-01-01', '2023-01-01')\n",
        "    data = df['Adj Close']\n",
        "    ticker = filename.split('/')[1].split('.')[0]\n",
        "\n",
        "    all_data[ticker] = data  # Store data for covariance calculation\n",
        "    # 3. Data Preprocessing\n",
        "    returns = data.pct_change().dropna()\n",
        "\n",
        "    # Reshape data for LSTM in a compatible format\n",
        "    X = returns.values.reshape(-1, 1, 1)\n",
        "\n",
        "    # 4. LSTM Modeling\n",
        "    class LSTMHyperModel(HyperModel):\n",
        "        def build(self, hp):\n",
        "            model = keras.Sequential()\n",
        "            model.add(layers.Input(shape=(1, 1)))\n",
        "            model.add(layers.LSTM(units=hp.Int('units', min_value=32, max_value=128, step=32), activation='relu'))\n",
        "            model.add(layers.Dense(1))\n",
        "            model.compile(optimizer=keras.optimizers.Adam(hp.Choice('learning_rate', [1e-2, 1e-3])), loss='mse')\n",
        "            return model\n",
        "\n",
        "    # Hyperparameter tuning\n",
        "    tuner = RandomSearch(\n",
        "        LSTMHyperModel(),\n",
        "        objective='val_loss',\n",
        "        max_trials=5,\n",
        "        executions_per_trial=3,\n",
        "        directory='lstm_tuning',\n",
        "        project_name=f'portfolio_optimization_{ticker}'\n",
        "    )\n",
        "\n",
        "    # Train the model only for the current ticker\n",
        "    tuner.search(X, returns.values, epochs=10, validation_split=0.2)\n",
        "    best_model = tuner.get_best_models(num_models=1)[0]\n",
        "\n",
        "    # Predict future returns\n",
        "    predicted_returns = best_model.predict(X)\n",
        "\n",
        "    # Aggregate predictions to get expected daily returns for each asset\n",
        "    expected_daily_return = np.mean(predicted_returns)\n",
        "\n",
        "    # Annualize the expected daily return\n",
        "    annualized_return = (1 + expected_daily_return) ** 252 - 1\n",
        "    all_expected_returns[ticker] = annualized_return\n",
        "\n",
        "    # Print annualized expected returns for each ticker\n",
        "    print(f\"Annualized Expected Returns for {ticker}: {annualized_return}\")\n",
        "    \n",
        "    filename = ticker + '_annualized_return.pkl'\n",
        "    with open(filename,'wb') as f:\n",
        "        pickle.dump(annualized_return,f)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "al6N1b8yK2br"
      },
      "outputs": [],
      "source": [
        "# Convert the dictionary to a Pandas Series\n",
        "expected_returns_series = pd.Series(all_expected_returns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qxSsh0jMI_zG",
        "outputId": "0ab34716-c669-4e36-ccc4-aaeb72feec1a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Optimized Portfolio Weights: OrderedDict([('tsla', 1.0), ('v', 0.0)])\n"
          ]
        }
      ],
      "source": [
        "# Portfolio Optimization\n",
        "# Calculate the covariance matrix using all tickers' data\n",
        "cov_matrix = risk_models.risk_matrix(all_data, method='ledoit_wolf' )\n",
        "# https://pyportfolioopt.readthedocs.io/en/latest/RiskModels.html\n",
        "ef = EfficientFrontier(expected_returns=expected_returns_series, cov_matrix=cov_matrix)\n",
        "weights = ef.max_sharpe()\n",
        "cleaned_weights = ef.clean_weights()\n",
        "\n",
        "# Print the optimized portfolio weights\n",
        "print(\"Optimized Portfolio Weights:\", cleaned_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open('cleaned_weights.pkl','wb') as f:\n",
        "    pickle.dump(cleaned_weights,f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OrderedDict([('tsla', 1.0), ('v', 0.0)])\n"
          ]
        }
      ],
      "source": [
        "with open('cleaned_weights.pkl','rb') as f:\n",
        "    load_dict = pickle.load(f)\n",
        "    \n",
        "print(load_dict)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
